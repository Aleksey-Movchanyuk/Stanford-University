.Platform$OS.type
if(.Platform$OS.type == "windows") {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
}
if(.Platform$OS.type == "windows") {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
}
source("warmUpExercise.R")
# ==================== Part 1: Basic Function ====================
A = warmUpExercise()
# ======================= Part 2: Plotting =======================
profits <- read.table(file="../ex1/ex1data1.txt", sep=",", dec=".")
colnames(profits) <- c("profit", "population")
library(ggplot2)
qplot(profit, population, data=profits, colour = I("red"))
install.packages("ggplot2")
# ======================= Part 2: Plotting =======================
profits <- read.table(file="../ex1/ex1data1.txt", sep=",", dec=".")
colnames(profits) <- c("profit", "population")
library(ggplot2)
qplot(profit, population, data=profits, colour = I("red"))
# =================== Part 3: Gradient descent ===================
X <- profits$profit; y <- profits$population;
m <- length(y); # number of training examples
X <- cbind(matrix(1, m), X) # Add a column of ones to X
theta <- matrix(0, 2); # initialize fitting parameters
# Some gradient descent settings
iterations <- 1500;
alpha <- 0.01;
source("computeCost.R")
source("gradientDescent.R")
# compute and display initial cost
computeCost(X, y, theta)
# run gradient descent
theta <- gradientDescent(X, y, theta, alpha, iterations);
# print theta to screen
sprintf('Theta found by gradient descent: ');
sprintf('%f %f \n', theta[1,], theta[2,]);
# Predict values for population sizes of 35,000 and 70,000
predict1 <- matrix(c(1, 3.5), nrow=1, ncol=2) %*% theta;
sprintf('For population = 35,000, we predict a profit of %f', predict1*10000);
predict2 = matrix(c(1, 7), nrow=1, ncol=2) %*% theta;
sprintf('For population = 70,000, we predict a profit of %f', predict2*10000);
# Plot the linear fit
# Gradient descent
lr <- data.frame( cbind( X[,2], X%*%theta ) )
colnames(lr) <- c("profit", "population")
# R linear regresion
fit <- lm(profit ~ population, data = profits)
lr_r <- data.frame( cbind( X[,2], X%*%matrix(coefficients(fit), nrow=2, ncol=1) ) )
colnames(lr_r) <- c("profit", "population")
ggplot() +
geom_point(data=profits, aes(x = profit, y = population, colour="Training data")) +
geom_line(data=lr, aes(x = profit, y = population, colour="Linear Regresion. Gradient Descent")) +
geom_line(data=lr_r, aes(x = profit, y = population, colour="Linear Regresion. R Function")) +
scale_colour_manual("",
values = c("Training data" = "red",
"Linear Regresion. Gradient Descent" = "blue",
"Linear Regresion. R Function" = "green")) +
labs(title = "Week #1. Linear Regresion") +
theme(legend.position = "right")
## Initialization
if(.Platform$OS.type == "windows") {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
}
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
View(data)
X <- data[,2:3];
X <- data[,1:2]; y <- data[,3];
X <- matrix(data[,1:2]); y <- data[,3];
View(X)
X <- matrix(data[,1:2], nrow=length(data), ncol=2);
View(X)
length(data)
X <- matrix(data[,1:2], nrow=length(data[,1]), ncol=2);
View(X)
X <- data[,1:2]; y <- data[,3];
X <- data.matrix(data[,1:2]); y <- data[,3];
View(X)
featureNormalize <- function (X_norm, mu, sigma) {
#FEATURENORMALIZE Normalizes the features in X
#   FEATURENORMALIZE(X) returns a normalized version of X where
#   the mean value of each feature is 0 and the standard deviation
#   is 1. This is often a good preprocessing step to do when
#   working with learning algorithms.
# You need to set these values correctly
X_norm <- X;
mu <- zeros(1, size(X, 2));
sigma <- zeros(1, size(X, 2));
# ====================== YOUR CODE HERE ======================
# Instructions: First, for each feature dimension, compute the mean
#               of the feature and subtract it from the dataset,
#               storing the mean value in mu. Next, compute the
#               standard deviation of each feature and divide
#               each feature by it's standard deviation, storing
#               the standard deviation in sigma.
#
#               Note that X is a matrix where each column is a
#               feature and each row is an example. You need
#               to perform the normalization separately for
#               each feature.
#
# Hint: You might find the 'mean' and 'std' functions useful.
#
# ============================================================
}
X_norm <- X;
mu <- matrix(0, size(X[, 2]));
mu <- matrix(0, length(X[, 2]));
View(mu)
mean(X[,2])
View(X)
mu <- matrix(0, nrow=1, ncol=length(X));
View(mu)
mu <- matrix(0, nrow=1, ncol=ncol(X));
X_norm <- X;
mu <- matrix(0, nrow=1, ncol=ncol(X));
sigma <- matrix(0, nrow=1, ncol=ncol(X));
for (i in 1:ncol(X)) {
mu[i] <- mean(X[,i])
}
std(X[,2])
sd(X[,2])
# compute the standard deviation of the features
for (i in 1:ncol(X)) {
sigma[i] <- sd(X[,i])
}
X_norm <- X %/% sigma;
X_norm <- X / sigma;
X_norm/2
X_norm - mu
X_norm %-% mu
for (i in 1:nrow(X)) {
for (j in 1:ncol(X)) {
X_norm[i, j] <- X[i, j] - mu[j] / sigma[j];
}
}
View(X_norm)
for (i in 1:nrow(X)) {
for (j in 1:ncol(X)) {
X_norm[i, j] <- (X[i, j] - mu[j]) / sigma[j];
}
}
View(X_norm)
View(X)
View(X_norm)
matrix(1, nrow(X))
X <- cbind(matrix(1, nrow(X)), X)
View(X)
## ================ Part 1: Feature Normalization ================
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
X <- data.matrix( data[,1:2] ); y <- data[,3];
m <- length(y); # number of training examples
X <- featureNormalize(X)
# Add intercept term to X
X <- cbind(matrix(1, nrow(X)), X)
## ================ Part 1: Feature Normalization ================
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
X <- data.matrix( data[,1:2] ); y <- data[,3];
m <- length(y); # number of training examples
source("featureNormalize.R")
X <- featureNormalize(X)
# Add intercept term to X
X <- cbind(matrix(1, nrow(X)), X)
View(X)
## Initialization
if(.Platform$OS.type == "windows") {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
}
## ================ Part 1: Feature Normalization ================
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
X <- data.matrix( data[,1:2] ); y <- data[,3];
m <- length(y); # number of training examples
# Scale features and set them to zero mean
source("featureNormalize.R")
X_norn <- featureNormalize(X)
# Add intercept term to X
X_norn <- cbind(matrix(1, nrow(X_norn)), X_norn)
## Machine Learning Online Class
#  Exercise 1: Linear regression with multiple variables
#
#  Instructions
#  ------------
#
#  This file contains code that helps you get started on the
#  linear regression exercise.
#
## Initialization
if(.Platform$OS.type == "windows") {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
}
## ================ Part 1: Feature Normalization ================
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
X <- data.matrix( data[,1:2] ); y <- data[,3];
m <- length(y); # number of training examples
# Scale features and set them to zero mean
source("featureNormalize.R")
X_norm <- featureNormalize(X)
# Add intercept term to X
X_norm <- cbind(matrix(1, nrow(X_norn)), X_norn)
## Machine Learning Online Class
#  Exercise 1: Linear regression with multiple variables
#
#  Instructions
#  ------------
#
#  This file contains code that helps you get started on the
#  linear regression exercise.
#
## Initialization
if(.Platform$OS.type == "windows") {
setwd("c:/Users/omovchaniuk/Documents/GitHub/Stanford-University/week 2/machine-learning-ex1/ex1 R")
} else {
setwd("/Users/aleksey/Documents/MOOC/Coursera/ML/Stanford University/week 2/machine-learning-ex1/ex1 R")
}
## ================ Part 1: Feature Normalization ================
# Load Data
data <- read.table(file="../ex1/ex1data2.txt", sep=",", dec=".")
X <- data.matrix( data[,1:2] ); y <- data[,3];
m <- length(y); # number of training examples
# Scale features and set them to zero mean
source("featureNormalize.R")
X_norm <- featureNormalize(X)
# Add intercept term to X
X_norm <- cbind(matrix(1, nrow(X_norm)), X_norm)
View(X_norm)
